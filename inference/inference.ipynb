{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc445254-daef-4686-91f0-f38871431fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5870\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [20/Apr/2025 07:18:11] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [20/Apr/2025 07:18:11] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [20/Apr/2025 07:18:22] \"POST /predict_video HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import os, io, base64, tempfile\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# ─── CONFIG ─────────────────────────────────────────────\n",
    "DEVICE          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CHECKPOINT_PATH = \"models/resnet_transformer.pth\"\n",
    "NUM_BINS        = 5\n",
    "IMG_H, IMG_W    = 180, 240\n",
    "\n",
    "# ─── MODEL ──────────────────────────────────────────────\n",
    "class ResNetTransformer(nn.Module):\n",
    "    def __init__(self, num_classes, num_bins):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        resnet.conv1 = nn.Conv2d(2,64,7,2,3,bias=False)\n",
    "        resnet.fc    = nn.Identity()\n",
    "        self.backbone  = resnet\n",
    "        d_model        = 2048\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1,num_bins,d_model))\n",
    "        enc_layer      = TransformerEncoderLayer(d_model,8,2048,dropout=0.1)\n",
    "        self.transformer = TransformerEncoder(enc_layer,num_layers=2)\n",
    "        self.classifier  = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B,K,2,H,W]\n",
    "        B,K,C,H,W = x.shape\n",
    "        x = x.view(B*K, C, H, W)\n",
    "        f = self.backbone(x)                    # [B*K,2048]\n",
    "        f = f.view(B,K,-1) + self.pos_embed     # [B,K,2048]\n",
    "        t = f.permute(1,0,2)                    # [K,B,2048]\n",
    "        out = self.transformer(t).mean(0)       # [B,2048]\n",
    "        return self.classifier(out)             # [B,num_classes]\n",
    "\n",
    "# ─── LOAD MODEL ──────────────────────────────────────────\n",
    "ckpt       = torch.load(CHECKPOINT_PATH, map_location=DEVICE, weights_only=True)\n",
    "label_map  = ckpt.get(\"label_map\", {i:f\"class_{i}\" for i in range(7)})\n",
    "state_dict = ckpt.get(\"model_state_dict\", ckpt)\n",
    "model = ResNetTransformer(len(label_map), NUM_BINS).to(DEVICE)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "\n",
    "# ─── HELPERS ─────────────────────────────────────────────\n",
    "def frame_to_voxel(img_pil):\n",
    "    arr = np.array(img_pil.resize((IMG_W,IMG_H))).astype(np.float32)\n",
    "    arr = (arr - arr.min())/(arr.max()-arr.min()+1e-6)\n",
    "    vox = np.stack([arr,arr],axis=0)[None]      # [1,2,H,W]\n",
    "    vox = np.repeat(vox, NUM_BINS, axis=0)      # [K,2,H,W]\n",
    "    return torch.from_numpy(vox).unsqueeze(0).to(DEVICE)  # [1,K,2,H,W]\n",
    "\n",
    "def extract_frame(video_path, timestamp=None):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"cannot open video\")\n",
    "    fps   = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if timestamp is None:\n",
    "        # default: middle of clip\n",
    "        frame_no = frame_count // 2\n",
    "    else:\n",
    "        frame_no = int(timestamp * fps)\n",
    "        frame_no = max(0, min(frame_no, frame_count-1))\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        raise RuntimeError(f\"couldn't read frame {frame_no}\")\n",
    "    # convert BGR → grayscale PIL\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    return Image.fromarray(gray)\n",
    "\n",
    "# ─── FLASK APP ───────────────────────────────────────────\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/predict_video\", methods=[\"POST\"])\n",
    "def predict_video():\n",
    "    \"\"\"\n",
    "    Expects a multipart/form-data:\n",
    "      - 'video' : the uploaded file\n",
    "      - 'time'  : optional float seconds at which to extract frame\n",
    "    \"\"\"\n",
    "    if 'video' not in request.files:\n",
    "        return jsonify(error=\"no video file\"), 400\n",
    "\n",
    "    vid = request.files['video']\n",
    "    ts  = request.form.get('time', type=float, default=None)\n",
    "\n",
    "    # save to temp file\n",
    "    suffix = os.path.splitext(vid.filename)[1]\n",
    "    with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as tmp:\n",
    "        vid.save(tmp.name)\n",
    "        tmp_path = tmp.name\n",
    "\n",
    "    try:\n",
    "        frame = extract_frame(tmp_path, timestamp=ts)\n",
    "        voxel = frame_to_voxel(frame)                 # [1,K,2,H,W]\n",
    "        with torch.no_grad():\n",
    "            logits = model(voxel)\n",
    "            probs  = torch.softmax(logits,1)[0].cpu().numpy()\n",
    "            idx    = int(probs.argmax())\n",
    "    finally:\n",
    "        os.unlink(tmp_path)\n",
    "\n",
    "    return jsonify({\n",
    "        \"emotion\":     label_map[idx],\n",
    "        \"confidence\":  float(probs[idx]),\n",
    "        \"all_probs\":   { label_map[i]: float(p) for i,p in enumerate(probs) }\n",
    "    })\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    app.run(port=5870)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
